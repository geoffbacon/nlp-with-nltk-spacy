{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy is a newer library than NLTK, specifically designed to i) work on larger problems and ii) hide irrelevant details from the users. Like NLTK, there's a lot to this library. We'll focus on the main features:\n",
    "\n",
    "[Loading spaCy models](#loading)<br>\n",
    "\n",
    "[Tokenization](#tokenization)<br>\n",
    "\n",
    "[Lemmatization](#lemma)<br>\n",
    "\n",
    "[Named entity recognition](#ner)<br>\n",
    "\n",
    "[Visualizing NER](#visualize-ner)<br>\n",
    "\n",
    "[Word vectors and similarity](#vectors)<br>\n",
    "\n",
    "### Time\n",
    "- Teaching: 30 minutes\n",
    "- Exercises: 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spaCy models <a id='loading'></a>\n",
    "\n",
    "spaCy required us to download a model, which we did in the second notebook. spaCy has different models for different languages. The models are what actually do the processing in spaCy. To make use of the models, we first load them in spaCy with `nlp = spacy.load('en')`, which stores the model in a variable called `nlp` for us. The `'en'` stands for English. You can see what other languages spaCy supports [here](https://spacy.io/usage/models). If you wanted to process data in those languages, you'd need to first download the relevant models and then load it in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of `nlp` as a function that we can apply to text data we want to analyze. First, let's read in the Python wikipedia page into a variable called `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "\n",
    "def read(fname):\n",
    "    fname = os.path.join(DATA_DIR, fname)\n",
    "    with open(fname) as f:\n",
    "        return f.read()\n",
    "\n",
    "text = read('python_wikipedia.txt')\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `nlp` model to process it. We call the `nlp` object on `text`. When we do this, spaCy does a lot of work behind the scenes. In fact, most of the processing that we'll use later on is done at this stage. spaCy analyzes the text, and stores the result in a special `Doc` object. By convention, we call this `doc`. The `Doc` object holds all the information that we'll use later on, such as the sentence boundaries, the POS tags, the named entities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization <a id='tokenization'></a>\n",
    "\n",
    "Tokenization in spaCy is easy. In fact, it's already done! When we iterate over a `Doc` object, spaCy assumes we want to iterate over the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `token` in `doc` is a `Token` object. This is an object that stores all the information about the token. To get the string representation of the token, we use the `.text` attribute. We'll see that all the information that we care about in spaCy is stored in attributes of objects like `Token`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token = doc[0]\n",
    "type(first_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the `doc` object how many tokens it has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the string representations of all the tokens in our text into a list called `tokens`. Check that it has 7547 strings in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "spaCy has already done the POS tagging for us. Guess where that information is stored? You got it: it's in an attribute of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two observations. First, these labels may be opaque to you. What does `PROPN` mean? And `ADP`? spaCy has got you covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain('PROPN'))\n",
    "print(spacy.explain('ADP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, if you looked closely at the attribute we used for getting the part of speech, you would have seen that we used `.pos_`, with the underscore at the end. What's that about?\n",
    "\n",
    "It all has to do with the fact that spaCy is designed to be hard-core, \"industrial-strength\" NLP software. It wants to be fast and efficient. To make it run fast, it actually stores a lot of the information as hashes, or special numbers, that refer to the more human-readable data. Think of them as unique codes for data. Storing these hashes is more efficient than using strings like `'PROPN'`. spaCy keeps the efficient data representation in attributes without the underscore, and keeps the human-readable form in an identically-named attribute with a trailing underscore. Most of the time, we as users of spaCy want the human-readable form, so we'll use the attributes with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text, token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get a list of all POS tags in the document. BONUS: Get a list of tuples of (word, pos) for every token in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization <a id='lemma'></a>\n",
    "\n",
    "No prizes for guessing where the lemmas for each token are stored. You'll notice that nowhere do we have to say what algorithm we want to use to get the lemmata (that's the plural of _lemma_). That's the whole point of spaCy. The designers don't want you to have to worry about what the best algorithm is to use. They have done their research, and chosen what they believe is a general-purpose method. This is pretty different from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:20]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition <a id='ner'></a>\n",
    "\n",
    "Named entity recognition (NER) is a big task in NLP. And rightfully so: it's really useful. A named entity is a \"real-world object\" that's assigned a name â€“ for example, a person, a country, a product or a book title. NER refers to extracting the named entities out of a text. Imagine if you're researching whether a particular newspaper is politically biased. One immediate thing you might ask is how often they talk about politicians of different persuasions. You could use NER to extract out all the mentions of people, filter them down to politicians, and classify them by party. Then you'd want to look at the number of mentions of people in each party. If one's much higher than the others, that could be because the newspaper is biased. (It could also be a million other things.)\n",
    "\n",
    "In NER, the different named entities that are extracted are grouped by their type. For example, \"person\", \"organization\", \"location\", \"country\", etc. In spaCy, there are lots of [different types](https://spacy.io/api/annotation#named-entities) of named entities that it can extract.\n",
    "\n",
    "Named entities in spaCy are available as the `ents` property of a `Doc`. The `.label_` tells us the type of named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ner_sentence = '''On Wednesday, Apple announced that it is looking to buy a U.K. startup called Bamboozle.\n",
    "It stated that it was willing to pay $1 billion for the rights to own its services in America, Vanuatu and Sweden.\n",
    "Although none of its employees speak fluent French or Swahili, Bamboozle offered to expand its services to both\n",
    "France and Tanzania. The rights cover the entirety of mainland U.S., except for Lake Michigan.'''.replace('\\n', ' ')\n",
    "\n",
    "example_ner_doc = nlp(example_ner_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in example_ner_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Extract all the named entities in the Python wikipedia page, currently stored in `doc`. Bonus: find the most popular person (i.e. the person with the most mentions) and the most popular country (country is labeled as 'GPE')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing NER <a id='visualize-ner'></a>\n",
    "\n",
    "spaCy has some cool features for visualizing its analysis of text data. To use this, we have to import `displacy` from the spacy library. We can ask `displacy` to `render` the NER information in `doc`, paying attention to tell it we're in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "\n",
    "Dependency parsing refers to drawing the relationships between individual words in a sentence. Just like NER, this is a huge topic in NLP. It's so big we're not going to cover it now. For our purposes, it's useful to know which words modify which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:18]:\n",
    "    print(token.text, token.dep_, token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not seem impressive. But let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_doc = nlp(text[:90])\n",
    "displacy.render(dependency_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "For the text of the Python wikipedia page, extract out the following information for each token:\n",
    "- string representation\n",
    "- pos\n",
    "- lemma\n",
    "- whether it's a stop word (`.is_stop`)\n",
    "- whether it's a punctuation symbol (`.is_punct`)\n",
    "- whether it's a number (`.like_num`)\n",
    "- the dependency relation (`.dep_`)\n",
    "\n",
    "Store this information in a list for each piece of information (i.e. a list for pos, a list for lemmata). BONUS: turn this into a `pandas.DataFrame` and find the distribution of pos, and the distribution of pos given the word is a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and similarity <a id='vectors'></a>\n",
    "\n",
    "Word vectors are mathematical representations of words. They allow us to find words that are similar to one another, and by extension, how similar texts are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'dog cat horse banana peach strawberry')\n",
    "data = []\n",
    "for token1 in tokens:\n",
    "    dic = {}\n",
    "    for token2 in tokens:\n",
    "        dic[token2] = token1.similarity(token2)\n",
    "    data.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, index=[t.text for t in tokens])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley = read('berkeley_wikipedia.txt')\n",
    "stanford = read('stanford_wikipedia.txt')\n",
    "mit = read('mit_wikipedia.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_doc = nlp(berkeley)\n",
    "stanford_doc = nlp(stanford)\n",
    "mit_doc = nlp(mit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_doc.similarity(stanford_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_doc.similarity(mit_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_doc.similarity(doc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
